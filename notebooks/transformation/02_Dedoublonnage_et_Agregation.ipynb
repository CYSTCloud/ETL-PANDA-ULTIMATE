{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation des données de pandémies - Partie 2: Dédoublonnage et Agrégation\n",
    "\n",
    "Ce notebook réalise la suppression des doublons et l'agrégation des données nettoyées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration initiale\n",
    "\n",
    "Importation des bibliothèques nécessaires et chargement des données nettoyées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration de la visualisation\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Définition des chemins vers les fichiers sources\n",
    "chemin_base = Path().resolve().parent.parent if 'notebooks' in str(Path().resolve()) else Path().resolve().parent\n",
    "dossier_donnees_transformees = chemin_base / \"donnees\" / \"transformees\"\n",
    "\n",
    "# Affichage du chemin\n",
    "print(f\"Dossier des données transformées: {dossier_donnees_transformees}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données nettoyées\n",
    "print(\"Chargement des données nettoyées...\")\n",
    "\n",
    "donnees = {}\n",
    "\n",
    "try:\n",
    "    # Chargement des données COVID-19 nettoyées\n",
    "    fichier_covid19 = dossier_donnees_transformees / \"covid19_nettoye.csv\"\n",
    "    if fichier_covid19.exists():\n",
    "        donnees[\"covid19\"] = pd.read_csv(fichier_covid19, parse_dates=[\"date\"])\n",
    "        print(f\"Données COVID-19 nettoyées chargées: {donnees['covid19'].shape[0]} lignes, {donnees['covid19'].shape[1]} colonnes\")\n",
    "    else:\n",
    "        print(f\"Impossible de charger les données COVID-19 nettoyées: fichier non trouvé\")\n",
    "    \n",
    "    # Chargement des données MPOX nettoyées\n",
    "    fichier_mpox = dossier_donnees_transformees / \"mpox_nettoye.csv\"\n",
    "    if fichier_mpox.exists():\n",
    "        donnees[\"mpox\"] = pd.read_csv(fichier_mpox, parse_dates=[\"date\"])\n",
    "        print(f\"Données MPOX nettoyées chargées: {donnees['mpox'].shape[0]} lignes, {donnees['mpox'].shape[1]} colonnes\")\n",
    "    else:\n",
    "        print(f\"Impossible de charger les données MPOX nettoyées: fichier non trouvé\")\n",
    "    \n",
    "    # Chargement des données Worldometer nettoyées\n",
    "    fichier_worldometer = dossier_donnees_transformees / \"worldometer_nettoye.csv\"\n",
    "    if fichier_worldometer.exists():\n",
    "        donnees[\"worldometer\"] = pd.read_csv(fichier_worldometer, parse_dates=[\"date\"])\n",
    "        print(f\"Données Worldometer nettoyées chargées: {donnees['worldometer'].shape[0]} lignes, {donnees['worldometer'].shape[1]} colonnes\")\n",
    "    else:\n",
    "        print(f\"Impossible de charger les données Worldometer nettoyées: fichier non trouvé\")\n",
    "    \n",
    "    print(\"\\nChargement des données nettoyées terminé avec succès!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement des données nettoyées: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Suppression des doublons dans les données COVID-19\n",
    "\n",
    "Identification et suppression des entrées en double dans les données COVID-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des doublons dans les données COVID-19\n",
    "if \"covid19\" in donnees:\n",
    "    print(\"Suppression des doublons dans les données COVID-19...\")\n",
    "    \n",
    "    # Affichage du nombre de lignes avant suppression des doublons\n",
    "    print(f\"Nombre de lignes avant suppression des doublons: {donnees['covid19'].shape[0]}\")\n",
    "    \n",
    "    # Vérification des doublons par pays, région et date\n",
    "    doublons = donnees['covid19'].duplicated(subset=[\"date\", \"pays\", \"region\"], keep=False)\n",
    "    print(f\"Nombre de doublons identifiés: {doublons.sum()}\")\n",
    "    \n",
    "    if doublons.sum() > 0:\n",
    "        print(\"\\nExemples de doublons:\")\n",
    "        display(donnees['covid19'][doublons].head())\n",
    "    \n",
    "    # Suppression des doublons en gardant la première occurrence\n",
    "    donnees['covid19'] = donnees['covid19'].drop_duplicates(subset=[\"date\", \"pays\", \"region\"], keep=\"first\")\n",
    "    \n",
    "    # Affichage du nombre de lignes après suppression des doublons\n",
    "    print(f\"Nombre de lignes après suppression des doublons: {donnees['covid19'].shape[0]}\")\n",
    "    print(\"Suppression des doublons dans les données COVID-19 terminée avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Suppression des doublons dans les données MPOX\n",
    "\n",
    "Identification et suppression des entrées en double dans les données MPOX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des doublons dans les données MPOX\n",
    "if \"mpox\" in donnees:\n",
    "    print(\"Suppression des doublons dans les données MPOX...\")\n",
    "    \n",
    "    # Affichage du nombre de lignes avant suppression des doublons\n",
    "    print(f\"Nombre de lignes avant suppression des doublons: {donnees['mpox'].shape[0]}\")\n",
    "    \n",
    "    # Vérification des doublons par pays et date\n",
    "    doublons = donnees['mpox'].duplicated(subset=[\"date\", \"pays\"], keep=False)\n",
    "    print(f\"Nombre de doublons identifiés: {doublons.sum()}\")\n",
    "    \n",
    "    if doublons.sum() > 0:\n",
    "        print(\"\\nExemples de doublons:\")\n",
    "        display(donnees['mpox'][doublons].head())\n",
    "    \n",
    "    # Suppression des doublons en gardant la première occurrence\n",
    "    donnees['mpox'] = donnees['mpox'].drop_duplicates(subset=[\"date\", \"pays\"], keep=\"first\")\n",
    "    \n",
    "    # Affichage du nombre de lignes après suppression des doublons\n",
    "    print(f\"Nombre de lignes après suppression des doublons: {donnees['mpox'].shape[0]}\")\n",
    "    print(\"Suppression des doublons dans les données MPOX terminée avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Suppression des doublons dans les données Worldometer\n",
    "\n",
    "Identification et suppression des entrées en double dans les données Worldometer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des doublons dans les données Worldometer\n",
    "if \"worldometer\" in donnees:\n",
    "    print(\"Suppression des doublons dans les données Worldometer...\")\n",
    "    \n",
    "    # Affichage du nombre de lignes avant suppression des doublons\n",
    "    print(f\"Nombre de lignes avant suppression des doublons: {donnees['worldometer'].shape[0]}\")\n",
    "    \n",
    "    # Vérification des doublons par pays et date\n",
    "    doublons = donnees['worldometer'].duplicated(subset=[\"date\", \"pays\"], keep=False)\n",
    "    print(f\"Nombre de doublons identifiés: {doublons.sum()}\")\n",
    "    \n",
    "    if doublons.sum() > 0:\n",
    "        print(\"\\nExemples de doublons:\")\n",
    "        display(donnees['worldometer'][doublons].head())\n",
    "    \n",
    "    # Suppression des doublons en gardant la première occurrence\n",
    "    donnees['worldometer'] = donnees['worldometer'].drop_duplicates(subset=[\"date\", \"pays\"], keep=\"first\")\n",
    "    \n",
    "    # Affichage du nombre de lignes après suppression des doublons\n",
    "    print(f\"Nombre de lignes après suppression des doublons: {donnees['worldometer'].shape[0]}\")\n",
    "    print(\"Suppression des doublons dans les données Worldometer terminée avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agrégation des données COVID-19\n",
    "\n",
    "Agrégation des données COVID-19 par pays et date pour réduire le volume de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrégation des données COVID-19\n",
    "if \"covid19\" in donnees:\n",
    "    print(\"Agrégation des données COVID-19...\")\n",
    "    \n",
    "    # Affichage du nombre de lignes avant agrégation\n",
    "    print(f\"Nombre de lignes avant agrégation: {donnees['covid19'].shape[0]}\")\n",
    "    \n",
    "    # Agrégation des données par pays et date\n",
    "    covid19_agg = donnees['covid19'].groupby([\"date\", \"pays\", \"type_pandemie\"]).agg({\n",
    "        \"cas_confirmes\": \"sum\",\n",
    "        \"deces\": \"sum\",\n",
    "        \"guerisons\": \"sum\",\n",
    "        \"cas_actifs\": \"sum\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Affichage du nombre de lignes après agrégation\n",
    "    print(f\"Nombre de lignes après agrégation: {covid19_agg.shape[0]}\")\n",
    "    print(\"\\nAperçu des données après agrégation:\")\n",
    "    display(covid19_agg.head())\n",
    "    \n",
    "    # Sauvegarde des données agrégées\n",
    "    donnees[\"covid19_agg\"] = covid19_agg\n",
    "    print(\"Agrégation des données COVID-19 terminée avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agrégation des données Worldometer\n",
    "\n",
    "Agrégation des données Worldometer par pays et date pour réduire le volume de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrégation des données Worldometer\n",
    "if \"worldometer\" in donnees:\n",
    "    print(\"Agrégation des données Worldometer...\")\n",
    "    \n",
    "    # Affichage du nombre de lignes avant agrégation\n",
    "    print(f\"Nombre de lignes avant agrégation: {donnees['worldometer'].shape[0]}\")\n",
    "    \n",
    "    # Agrégation des données par pays et date\n",
    "    worldometer_agg = donnees['worldometer'].groupby([\"date\", \"pays\", \"type_pandemie\"]).agg({\n",
    "        \"cas_confirmes\": \"sum\",\n",
    "        \"deces\": \"sum\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Affichage du nombre de lignes après agrégation\n",
    "    print(f\"Nombre de lignes après agrégation: {worldometer_agg.shape[0]}\")\n",
    "    print(\"\\nAperçu des données après agrégation:\")\n",
    "    display(worldometer_agg.head())\n",
    "    \n",
    "    # Sauvegarde des données agrégées\n",
    "    donnees[\"worldometer_agg\"] = worldometer_agg\n",
    "    print(\"Agrégation des données Worldometer terminée avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fusion des données COVID-19\n",
    "\n",
    "Fusion des données COVID-19 provenant des différentes sources pour obtenir un jeu de données unifié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion des données COVID-19\n",
    "if \"covid19_agg\" in donnees and \"worldometer_agg\" in donnees:\n",
    "    print(\"Fusion des données COVID-19...\")\n",
    "    \n",
    "    # Sélection des colonnes communes\n",
    "    covid19_fusion = donnees[\"covid19_agg\"][[\"date\", \"pays\", \"type_pandemie\", \"cas_confirmes\", \"deces\"]]\n",
    "    worldometer_fusion = donnees[\"worldometer_agg\"][[\"date\", \"pays\", \"type_pandemie\", \"cas_confirmes\", \"deces\"]]\n",
    "    \n",
    "    # Ajout d'une colonne pour la source des données\n",
    "    covid19_fusion[\"source\"] = \"covid19\"\n",
    "    worldometer_fusion[\"source\"] = \"worldometer\"\n",
    "    \n",
    "    # Fusion des données\n",
    "    covid19_fusionne = pd.concat([covid19_fusion, worldometer_fusion])\n",
    "    \n",
    "    # Affichage du nombre de lignes après fusion\n",
    "    print(f\"Nombre de lignes après fusion: {covid19_fusionne.shape[0]}\")\n",
    "    print(\"\\nAperçu des données après fusion:\")\n",
    "    display(covid19_fusionne.head())\n",
    "    \n",
    "    # Vérification des doublons après fusion\n",
    "    doublons = covid19_fusionne.duplicated(subset=[\"date\", \"pays\"], keep=False)\n",
    "    print(f\"Nombre de doublons après fusion: {doublons.sum()}\")\n",
    "    \n",
    "    # Agrégation des données fusionnées par pays, date et type de pandémie\n",
    "    covid19_fusionne_agg = covid19_fusionne.groupby([\"date\", \"pays\", \"type_pandemie\"]).agg({\n",
    "        \"cas_confirmes\": \"max\",  # On prend le maximum pour éviter les sous-estimations\n",
    "        \"deces\": \"max\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Affichage du nombre de lignes après agrégation finale\n",
    "    print(f\"Nombre de lignes après agrégation finale: {covid19_fusionne_agg.shape[0]}\")\n",
    "    print(\"\\nAperçu des données après agrégation finale:\")\n",
    "    display(covid19_fusionne_agg.head())\n",
    "    \n",
    "    # Sauvegarde des données fusionnées\n",
    "    donnees[\"covid19_fusionne\"] = covid19_fusionne_agg\n",
    "    print(\"Fusion des données COVID-19 terminée avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Création du jeu de données final\n",
    "\n",
    "Création du jeu de données final en combinant les données COVID-19 fusionnées et les données MPOX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du jeu de données final\n",
    "if \"covid19_fusionne\" in donnees and \"mpox\" in donnees:\n",
    "    print(\"Création du jeu de données final...\")\n",
    "    \n",
    "    # Sélection des colonnes pertinentes pour les données MPOX\n",
    "    mpox_final = donnees[\"mpox\"][[\"date\", \"pays\", \"type_pandemie\", \"cas_confirmes\", \"deces\"]]\n",
    "    \n",
    "    # Combinaison des données COVID-19 et MPOX\n",
    "    donnees_final = pd.concat([donnees[\"covid19_fusionne\"], mpox_final])\n",
    "    \n",
    "    # Affichage du nombre de lignes du jeu de données final\n",
    "    print(f\"Nombre de lignes du jeu de données final: {donnees_final.shape[0]}\")\n",
    "    print(\"\\nAperçu du jeu de données final:\")\n",
    "    display(donnees_final.head())\n",
    "    \n",
    "    # Vérification des valeurs manquantes\n",
    "    valeurs_manquantes = donnees_final.isnull().sum()\n",
    "    print(\"\\nValeurs manquantes dans le jeu de données final:\")\n",
    "    display(valeurs_manquantes[valeurs_manquantes > 0])\n",
    "    \n",
    "    # Gestion des valeurs manquantes\n",
    "    donnees_final = donnees_final.fillna(0)\n",
    "    \n",
    "    # Conversion des types de données\n",
    "    donnees_final[\"cas_confirmes\"] = donnees_final[\"cas_confirmes\"].astype(int)\n",
    "    donnees_final[\"deces\"] = donnees_final[\"deces\"].astype(int)\n",
    "    \n",
    "    # Sauvegarde du jeu de données final\n",
    "    donnees[\"donnees_final\"] = donnees_final\n",
    "    print(\"Création du jeu de données final terminée avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sauvegarde du jeu de données final\n",
    "\n",
    "Sauvegarde du jeu de données final pour l'étape suivante de normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du jeu de données final\n",
    "if \"donnees_final\" in donnees:\n",
    "    print(\"Sauvegarde du jeu de données final...\")\n",
    "    \n",
    "    try:\n",
    "        # Sauvegarde du jeu de données final\n",
    "        chemin_fichier = dossier_donnees_transformees / \"donnees_final.csv\"\n",
    "        donnees[\"donnees_final\"].to_csv(chemin_fichier, index=False)\n",
    "        print(f\"Jeu de données final sauvegardé: {chemin_fichier}\")\n",
    "        print(\"\\nSauvegarde du jeu de données final terminée avec succès!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sauvegarde du jeu de données final: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons réalisé la suppression des doublons et l'agrégation des données nettoyées. Les principales opérations ont été :\n",
    "\n",
    "1. Suppression des doublons dans les données COVID-19, MPOX et Worldometer\n",
    "2. Agrégation des données par pays et date pour réduire le volume de données\n",
    "3. Fusion des données COVID-19 provenant des différentes sources\n",
    "4. Création du jeu de données final en combinant les données COVID-19 et MPOX\n",
    "\n",
    "Le jeu de données final a été sauvegardé dans un fichier CSV pour l'étape suivante de normalisation, qui sera réalisée dans le prochain notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
